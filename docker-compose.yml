version: "3.8"

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.3
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__WEBSERVER__RBAC: "True"
    _PIP_ADDITIONAL_REQUIREMENTS: "google-cloud-storage google-cloud-bigquery pandas great_expectations python-snappy praw requests dbt-core dbt-bigquery"
    GOOGLE_APPLICATION_CREDENTIALS: "/opt/airflow/gcp/sa.json"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./transform:/opt/airflow/transform
    - ./dbt:/opt/airflow/dbt
    - ./gcp:/opt/airflow/gcp
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User || true"

  webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      - airflow-init
      - scheduler

  scheduler:
    <<: *airflow-common
    command: scheduler

volumes:
  postgres-db-volume: